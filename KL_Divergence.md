# KL Divergence


- 用來驗證**兩個機率分佈差異性的指標**
- 因為其驗證不同分佈差異性的特性，適合應用在 loss-function 的設計上，可以在 backpropagation 的過程中，衡量預測解果與真實資料的相似程度


離散 KL Distance
: $D_{KL}(P||Q)=-\displaystyle\sum_{i}^{}P(i)ln\dfrac{Q(i)}{P(i)}$
: $D_{KL}(P||Q)=\displaystyle\sum_{i}^{}P(i)ln\dfrac{P(i)}{Q(i)}$

P
: 數據的真實分佈

Q
: 數據的理論分佈、估計的模型分佈、P的近似分佈

特性
: KL Distance 不具有對稱性
: $D_{KL}(P||Q)\neq D_{KL}(Q||P)$
: 從分布 $P$ 到 $Q$ 的距離通常並不等於從 $Q$ 到 $P$ 的距離

---

- 如果有兩個獨立的機率分布 $p(x)$ 和 $q(x)$ 同時對應到同一個隨機變數 $x$，也就是它們所在的空間是一樣的，則可以使用KL Divergence來測量這兩個分布的差異程度
- KL-Distance 描述的是用預測結果 $Q$ 分佈透過編碼轉換後可以用來表達 $P$ 分佈所需的編碼複雜度
- 假設今天預測結果跟 $P$ 一模一樣，顯然用 $Q$ 來表達 $P$ 是不需要經過任何的編碼轉換
- 但如果 P 跟 Q 之間的資訊亂度是很大的，要用 Q 來表示 P，轉換的過程想必是非常複雜，所需要的資訊量也更大
- 以 encode 的複雜度來看的話，那麼 P 轉換到 Q 跟 Q 轉換到 P 這兩件事的複雜度就不見得會相等，有可能正推是比較容易的，反推搞不好就很複雜

